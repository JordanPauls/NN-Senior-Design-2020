{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"co..., outputs=Tensor(\"fl...)`\n",
      "C:\\Users\\Will\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:85: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"co..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 3126 , Acc: 0.6118000149726868\n"
     ]
    }
   ],
   "source": [
    "#LENET_SLR_A\n",
    "from keras import backend\n",
    "from keras import datasets\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.models import Model,Sequential, model_from_json\n",
    "from keras.layers import Dense, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "fc_id = 5 # FC Layer Number\n",
    "rank = 6\n",
    "sr = 0.5\n",
    "rr = 0.5\n",
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1:]\n",
    "\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 32,32,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32,32,1) \n",
    "\n",
    "json_file = open('Lenet.json', 'r')\n",
    "lenet_model_json = json_file.read()\n",
    "json_file.close()\n",
    "lenet_model = model_from_json(lenet_model_json)\n",
    "lenet_model.load_weights(\"Lenet.h5\")\n",
    "lenet_model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "\n",
    "def sparse_SVD_ar(U,S,V,inp_act,out_act,keep,sr,rr):\n",
    "    \n",
    "    tU, tS, tV = U[:, 0:keep], S[0:keep], V[0:keep, :]\n",
    "\n",
    "    # Input node selection\n",
    "    iwm = np.sum(abs(inp_act),axis=0)\n",
    "    imid = sorted(iwm)[int(U.shape[0]*sr)]\n",
    "    ipl = np.where(iwm<imid)[0]\n",
    "\n",
    "    # Output node selection\n",
    "    owm = np.sum(abs(out_act),axis=0)\n",
    "    omid = sorted(owm)[int(V.shape[1]*sr)]\n",
    "    opl = np.where(owm<omid)[0]\n",
    "    \n",
    "    # Masking the weights\n",
    "    subrank = int(keep*rr)\n",
    "    for ind in ipl:\n",
    "        tU[ind,subrank:]=0\n",
    "\n",
    "    for ind in opl:\n",
    "        tV[subrank:,ind]=0\n",
    "\n",
    "    return tU, tS, tV\n",
    "\n",
    "# Loading Model\n",
    "lenet_model.load_weights(\"Lenet.h5\")\n",
    "lenet_model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "act_fc1 = Model(input=lenet_model.input, output=lenet_model.get_layer('flatten_2').output)\n",
    "act_fc2 = Model(input=lenet_model.input, output=lenet_model.get_layer('dense_4').output)\n",
    "\n",
    "# Flatten \n",
    "inp_act = act_fc1.predict(x_train)\n",
    "out_act = act_fc2.predict(x_train)\n",
    "\n",
    "keep = rank\n",
    "\n",
    "fc1 = lenet_model.layers[fc_id].get_weights()\n",
    "\n",
    "weights = np.copy(fc1)\n",
    "\n",
    "# Decomposition and Reconstruction\n",
    "U, S, V = np.linalg.svd(weights[0], full_matrices=False)\n",
    "tU, tS, tV = sparse_SVD_ar(U,S,V,inp_act,out_act,keep,sr,rr)\n",
    "\n",
    "weights_t = np.matmul(np.matmul(tU, np.diag(tS)), tV)\n",
    "\n",
    "# Loading weights for new model\n",
    "weights[0] = weights_t\n",
    "lenet_model.layers[fc_id].set_weights(weights)\n",
    "\n",
    "# Write the testing input and output variables\n",
    "score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "slr_accuracy = score[1]\n",
    "print('Params:',tU.size+tS.size+tV.size,', Acc:', slr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 0\n",
      "In 1\n",
      "In 2\n",
      "In 3\n",
      "In 4\n",
      "In 5\n",
      "In 6\n",
      "In 7\n",
      "In 8\n",
      "In 9\n",
      "In 10\n",
      "In 11\n",
      "In 12\n",
      "In 13\n",
      "In 14\n",
      "In 15\n",
      "In 16\n",
      "In 17\n",
      "In 18\n",
      "In 19\n",
      "In 20\n",
      "In 21\n",
      "In 22\n",
      "In 23\n",
      "In 24\n",
      "In 25\n",
      "In 26\n",
      "In 27\n",
      "In 28\n",
      "In 29\n",
      "In 30\n",
      "In 31\n",
      "In 32\n",
      "In 33\n",
      "In 34\n",
      "In 35\n",
      "In 36\n",
      "In 37\n",
      "In 38\n",
      "In 39\n",
      "In 40\n",
      "In 41\n",
      "In 42\n",
      "In 43\n",
      "In 44\n",
      "In 45\n",
      "In 46\n",
      "In 47\n",
      "In 48\n",
      "In 49\n",
      "In 50\n",
      "In 51\n",
      "In 52\n",
      "In 53\n",
      "In 54\n",
      "In 55\n",
      "In 56\n",
      "In 57\n",
      "In 58\n",
      "In 59\n",
      "In 60\n",
      "In 61\n",
      "In 62\n",
      "In 63\n",
      "In 64\n",
      "In 65\n",
      "In 66\n",
      "In 67\n",
      "In 68\n",
      "In 69\n",
      "In 70\n",
      "In 71\n",
      "In 72\n",
      "In 73\n",
      "In 74\n",
      "In 75\n",
      "In 76\n",
      "In 77\n",
      "In 78\n",
      "In 79\n",
      "In 80\n",
      "In 81\n",
      "In 82\n",
      "In 83\n",
      "In 84\n",
      "In 85\n",
      "In 86\n",
      "In 87\n",
      "In 88\n",
      "In 89\n",
      "In 90\n",
      "In 91\n",
      "In 92\n",
      "In 93\n",
      "In 94\n",
      "In 95\n",
      "In 96\n",
      "In 97\n",
      "In 98\n",
      "In 99\n",
      "In 100\n",
      "In 101\n",
      "In 102\n",
      "In 103\n",
      "In 104\n",
      "In 105\n",
      "In 106\n",
      "In 107\n",
      "In 108\n",
      "In 109\n",
      "In 110\n",
      "In 111\n",
      "In 112\n",
      "In 113\n",
      "In 114\n",
      "In 115\n",
      "In 116\n",
      "In 117\n",
      "In 118\n",
      "In 119\n",
      "In 120\n",
      "In 121\n",
      "In 122\n",
      "In 123\n",
      "In 124\n",
      "In 125\n",
      "In 126\n",
      "In 127\n",
      "In 128\n",
      "In 129\n",
      "In 130\n",
      "In 131\n",
      "In 132\n",
      "In 133\n",
      "In 134\n",
      "In 135\n",
      "In 136\n",
      "In 137\n",
      "In 138\n",
      "In 139\n",
      "In 140\n",
      "In 141\n",
      "In 142\n",
      "In 143\n",
      "In 144\n",
      "In 145\n",
      "In 146\n",
      "In 147\n",
      "In 148\n",
      "In 149\n",
      "In 150\n",
      "In 151\n",
      "In 152\n",
      "In 153\n",
      "In 154\n",
      "In 155\n",
      "In 156\n",
      "In 157\n",
      "In 158\n",
      "In 159\n",
      "In 160\n",
      "In 161\n",
      "In 162\n",
      "In 163\n",
      "In 164\n",
      "In 165\n",
      "In 166\n",
      "In 167\n",
      "In 168\n",
      "In 169\n",
      "In 170\n",
      "In 171\n",
      "In 172\n",
      "In 173\n",
      "In 174\n",
      "In 175\n",
      "In 176\n",
      "In 177\n",
      "In 178\n",
      "In 179\n",
      "In 180\n",
      "In 181\n",
      "In 182\n",
      "In 183\n",
      "In 184\n",
      "In 185\n",
      "In 186\n",
      "In 187\n",
      "In 188\n",
      "In 189\n",
      "In 190\n",
      "In 191\n",
      "In 192\n",
      "In 193\n",
      "In 194\n",
      "In 195\n",
      "In 196\n",
      "In 197\n",
      "In 198\n",
      "In 199\n",
      "In 200\n",
      "In 201\n",
      "In 202\n",
      "In 203\n",
      "In 204\n",
      "In 205\n",
      "In 206\n",
      "In 207\n",
      "In 208\n",
      "In 209\n",
      "In 210\n",
      "In 211\n",
      "In 212\n",
      "In 213\n",
      "In 214\n",
      "In 215\n",
      "In 216\n",
      "In 217\n",
      "In 218\n",
      "In 219\n",
      "In 220\n",
      "In 221\n",
      "In 222\n",
      "In 223\n",
      "In 224\n",
      "In 225\n",
      "In 226\n",
      "In 227\n",
      "In 228\n",
      "In 229\n",
      "In 230\n",
      "In 231\n",
      "In 232\n",
      "In 233\n",
      "In 234\n",
      "In 235\n",
      "In 236\n",
      "In 237\n",
      "In 238\n",
      "In 239\n",
      "In 240\n",
      "In 241\n",
      "In 242\n",
      "In 243\n",
      "In 244\n",
      "In 245\n",
      "In 246\n",
      "In 247\n",
      "In 248\n",
      "In 249\n",
      "In 250\n",
      "In 251\n",
      "In 252\n",
      "In 253\n",
      "In 254\n",
      "In 255\n",
      "In 256\n",
      "In 257\n",
      "In 258\n",
      "In 259\n",
      "In 260\n",
      "In 261\n",
      "In 262\n",
      "In 263\n",
      "In 264\n",
      "In 265\n",
      "In 266\n",
      "In 267\n",
      "In 268\n",
      "In 269\n",
      "In 270\n",
      "In 271\n",
      "In 272\n",
      "In 273\n",
      "In 274\n",
      "In 275\n",
      "In 276\n",
      "In 277\n",
      "In 278\n",
      "In 279\n",
      "In 280\n",
      "In 281\n",
      "In 282\n",
      "In 283\n",
      "In 284\n",
      "In 285\n",
      "In 286\n",
      "In 287\n",
      "In 288\n",
      "In 289\n",
      "In 290\n",
      "In 291\n",
      "In 292\n",
      "In 293\n",
      "In 294\n",
      "In 295\n",
      "In 296\n",
      "In 297\n",
      "In 298\n",
      "In 299\n",
      "In 300\n",
      "In 301\n",
      "In 302\n",
      "In 303\n",
      "In 304\n",
      "In 305\n",
      "In 306\n",
      "In 307\n",
      "In 308\n",
      "In 309\n",
      "In 310\n",
      "In 311\n",
      "In 312\n",
      "In 313\n",
      "In 314\n",
      "In 315\n",
      "In 316\n",
      "In 317\n",
      "In 318\n",
      "In 319\n",
      "In 320\n",
      "In 321\n",
      "In 322\n",
      "In 323\n",
      "In 324\n",
      "In 325\n",
      "In 326\n",
      "In 327\n",
      "In 328\n",
      "In 329\n",
      "In 330\n",
      "In 331\n",
      "In 332\n",
      "In 333\n",
      "In 334\n",
      "In 335\n",
      "In 336\n",
      "In 337\n",
      "In 338\n",
      "In 339\n",
      "In 340\n",
      "In 341\n",
      "In 342\n",
      "In 343\n",
      "In 344\n",
      "In 345\n",
      "In 346\n",
      "In 347\n",
      "In 348\n",
      "In 349\n",
      "In 350\n",
      "In 351\n",
      "In 352\n",
      "In 353\n",
      "In 354\n",
      "In 355\n",
      "In 356\n",
      "In 357\n",
      "In 358\n",
      "In 359\n",
      "In 360\n",
      "In 361\n",
      "In 362\n",
      "In 363\n",
      "In 364\n",
      "In 365\n",
      "In 366\n",
      "In 367\n",
      "In 368\n",
      "In 369\n",
      "In 370\n",
      "In 371\n",
      "In 372\n",
      "In 373\n",
      "In 374\n",
      "In 375\n",
      "In 376\n",
      "In 377\n",
      "In 378\n",
      "In 379\n",
      "In 380\n",
      "In 381\n",
      "In 382\n",
      "In 383\n",
      "In 384\n",
      "In 385\n",
      "In 386\n",
      "In 387\n",
      "In 388\n",
      "In 389\n",
      "In 390\n",
      "In 391\n",
      "In 392\n",
      "In 393\n",
      "In 394\n",
      "In 395\n",
      "In 396\n",
      "In 397\n",
      "In 398\n",
      "In 399\n",
      "Out 0\n",
      "Out 1\n",
      "Out 2\n",
      "Out 3\n",
      "Out 4\n",
      "Out 5\n",
      "Out 6\n",
      "Out 7\n",
      "Out 8\n",
      "Out 9\n",
      "Out 10\n",
      "Out 11\n",
      "Out 12\n",
      "Out 13\n",
      "Out 14\n",
      "Out 15\n",
      "Out 16\n",
      "Out 17\n",
      "Out 18\n",
      "Out 19\n",
      "Out 20\n",
      "Out 21\n",
      "Out 22\n",
      "Out 23\n",
      "Out 24\n",
      "Out 25\n",
      "Out 26\n",
      "Out 27\n",
      "Out 28\n",
      "Out 29\n",
      "Out 30\n",
      "Out 31\n",
      "Out 32\n",
      "Out 33\n",
      "Out 34\n",
      "Out 35\n",
      "Out 36\n",
      "Out 37\n",
      "Out 38\n",
      "Out 39\n",
      "Out 40\n",
      "Out 41\n",
      "Out 42\n",
      "Out 43\n",
      "Out 44\n",
      "Out 45\n",
      "Out 46\n",
      "Out 47\n",
      "Out 48\n",
      "Out 49\n",
      "Out 50\n",
      "Out 51\n",
      "Out 52\n",
      "Out 53\n",
      "Out 54\n",
      "Out 55\n",
      "Out 56\n",
      "Out 57\n",
      "Out 58\n",
      "Out 59\n",
      "Out 60\n",
      "Out 61\n",
      "Out 62\n",
      "Out 63\n",
      "Out 64\n",
      "Out 65\n",
      "Out 66\n",
      "Out 67\n",
      "Out 68\n",
      "Out 69\n",
      "Out 70\n",
      "Out 71\n",
      "Out 72\n",
      "Out 73\n",
      "Out 74\n",
      "Out 75\n",
      "Out 76\n",
      "Out 77\n",
      "Out 78\n",
      "Out 79\n",
      "Out 80\n",
      "Out 81\n",
      "Out 82\n",
      "Out 83\n",
      "Out 84\n",
      "Out 85\n",
      "Out 86\n",
      "Out 87\n",
      "Out 88\n",
      "Out 89\n",
      "Out 90\n",
      "Out 91\n",
      "Out 92\n",
      "Out 93\n",
      "Out 94\n",
      "Out 95\n",
      "Out 96\n",
      "Out 97\n",
      "Out 98\n",
      "Out 99\n",
      "Out 100\n",
      "Out 101\n",
      "Out 102\n",
      "Out 103\n",
      "Out 104\n",
      "Out 105\n",
      "Out 106\n",
      "Out 107\n",
      "Out 108\n",
      "Out 109\n",
      "Out 110\n",
      "Out 111\n",
      "Out 112\n",
      "Out 113\n",
      "Out 114\n",
      "Out 115\n",
      "Out 116\n",
      "Out 117\n",
      "Out 118\n",
      "Out 119\n",
      "Params: 3126 , Acc: 0.6833999752998352\n"
     ]
    }
   ],
   "source": [
    "#LENET_SLR_C\n",
    "from keras import backend\n",
    "from keras import datasets\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.models import Model,Sequential, model_from_json\n",
    "from keras.layers import Dense, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "fc_id = 5 # FC Layer Number\n",
    "rank = 6\n",
    "sr = 0.5\n",
    "rr = 0.5\n",
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1:]\n",
    "\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 32,32,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32,32,1) \n",
    "\n",
    "json_file = open('Lenet.json', 'r')\n",
    "lenet_model_json = json_file.read()\n",
    "json_file.close()\n",
    "lenet_model = model_from_json(lenet_model_json)\n",
    "lenet_model.load_weights(\"Lenet.h5\")\n",
    "lenet_model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "keep = rank\n",
    "fc1 = lenet_model.layers[fc_id].get_weights()\n",
    "\n",
    "weights = fc1[0]\n",
    "\n",
    "ip_node = weights.shape[0]\n",
    "op_node = weights.shape[1]\n",
    "\n",
    "U, S, V = np.linalg.svd(weights, full_matrices=False)\n",
    "tU, tS, tV = U[:, 0:keep], S[0:keep], V[0:keep, :]\n",
    "\n",
    "fc1[0] = np.matmul(np.matmul(tU, np.diag(tS)), tV)\n",
    "lenet_model.layers[fc_id].set_weights(fc1)\n",
    "\n",
    "score = lenet_model.evaluate(x_train, y_train, verbose=0)\n",
    "svd_cost = score[0]\n",
    "\n",
    "subrank = int(keep*rr)\n",
    "\n",
    "imc = np.zeros([ip_node])\n",
    "\n",
    "for ind in range(0,ip_node):\n",
    "    tempU = np.copy(tU)\n",
    "    tempU[ind,subrank:]=0\n",
    "    weights_t = np.copy(fc1)\n",
    "    weights_t[0] = np.matmul(np.matmul(tempU, np.diag(tS)), tV)\n",
    "    lenet_model.layers[fc_id].set_weights(weights_t)\n",
    "    score = lenet_model.evaluate(x_train, y_train, verbose=0)\n",
    "    svd_cost_t = score[0]\n",
    "    imc[ind] = abs(svd_cost-svd_cost_t)\n",
    "    lenet_model.layers[fc_id].set_weights(fc1)\n",
    "    print(\"In\",ind)\n",
    "    \n",
    "omc = np.zeros([op_node])\n",
    "\n",
    "for ind in range(0,op_node):\n",
    "    temptV = np.copy(tV)\n",
    "    temptV[subrank:,ind]=0\n",
    "    weights_t = np.copy(fc1)\n",
    "    weights_t[0] = np.matmul(np.matmul(tU, np.diag(tS)), temptV)\n",
    "    lenet_model.layers[fc_id].set_weights(weights_t)\n",
    "    score = lenet_model.evaluate(x_train, y_train, verbose=0)\n",
    "    svd_cost_t = score[0]\n",
    "    omc[ind] = abs(svd_cost-svd_cost_t)\n",
    "    lenet_model.layers[fc_id].set_weights(fc1)\n",
    "    print(\"Out\",ind)\n",
    "    \n",
    "# Loading Model\n",
    "lenet_model.load_weights(\"Lenet.h5\")\n",
    "lenet_model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "# Loading weights of the model\n",
    "fc1 = lenet_model.layers[fc_id].get_weights()\n",
    "\n",
    "weights = np.copy(fc1)\n",
    "U, S, V = np.linalg.svd(weights[0], full_matrices=False)\n",
    "tU, tS, tV = U[:, 0:keep], S[0:keep], V[0:keep, :]\n",
    "\n",
    "# Input node selection\n",
    "imid = sorted(imc)[int(ip_node*sr)]\n",
    "ipl = np.where(imc<imid)[0]\n",
    "\n",
    "# Output node selection\n",
    "omid = sorted(omc)[int(op_node*sr)]\n",
    "opl = np.where(omc<omid)[0]\n",
    "\n",
    "# Masking the weights\n",
    "subrank = int(keep*rr)\n",
    "for ind in ipl:\n",
    "    tU[ind,subrank:]=0\n",
    "\n",
    "for ind in opl:\n",
    "    tV[subrank:,ind]=0\n",
    "    \n",
    "weights_t = np.matmul(np.matmul(tU, np.diag(tS)), tV)\n",
    "\n",
    "# Loading weights for new model\n",
    "weights[0] = weights_t\n",
    "lenet_model.layers[fc_id].set_weights(weights)\n",
    "\n",
    "# Write the testing input and output variables\n",
    "score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "slr_accuracy = score[1]\n",
    "print('Params:',tU.size+tS.size+tV.size,', Acc:', slr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenet model from disk\n",
      "Params: 3126 , Acc: 0.6793000102043152\n"
     ]
    }
   ],
   "source": [
    "#LENET_SLR_W\n",
    "from keras import backend\n",
    "from keras import datasets\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.models import Model,Sequential, model_from_json\n",
    "from keras.layers import Dense, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "fc_id = 5 # FC Layer Number\n",
    "rank = 6\n",
    "sr = 0.5\n",
    "rr = 0.5\n",
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1:]\n",
    "\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 32,32,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32,32,1) \n",
    "\n",
    "json_file = open('Lenet.json', 'r')\n",
    "lenet_model_json = json_file.read()\n",
    "json_file.close()\n",
    "lenet_model = model_from_json(lenet_model_json)\n",
    "lenet_model.load_weights(\"Lenet.h5\")\n",
    "lenet_model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "print(\"lenet model from disk\")\n",
    "\n",
    "def sparse_SVD_wr(weights,U,S,V,keep,rate,subrank_rate):\n",
    "    tU, tS, tV = U[:, 0:keep], S[0:keep], V[0:keep, :]\n",
    "\n",
    "    # Input node selection\n",
    "    iwm = np.sum(abs(weights),axis=1)\n",
    "    imid = sorted(iwm)[int(weights.shape[0]*rate)]\n",
    "    ipl = np.where(iwm<imid)[0]\n",
    "\n",
    "    # Output node selection\n",
    "    owm = np.sum(abs(weights),axis=0)\n",
    "    omid = sorted(owm)[int(weights.shape[1]*rate)]\n",
    "    opl = np.where(owm<omid)[0]\n",
    "\n",
    "    # Masking the weights\n",
    "    subrank = int(keep*subrank_rate)\n",
    "    for ind in ipl:\n",
    "        tU[ind,subrank:]=0\n",
    "\n",
    "    for ind in opl:\n",
    "        tV[subrank:,ind]=0\n",
    "\n",
    "    return tU, tS, tV\n",
    "\n",
    "\n",
    "keep = rank\n",
    "fc1 = lenet_model.layers[fc_id].get_weights()\n",
    "\n",
    "weights = np.copy(fc1)\n",
    "\n",
    "# Decomposition and Reconstruction\n",
    "U, S, V = np.linalg.svd(weights[0], full_matrices=False)\n",
    "tU, tS, tV = sparse_SVD_wr(weights[0],U,S,V,keep,sr,rr)\n",
    "\n",
    "weights_t = np.matmul(np.matmul(tU, np.diag(tS)), tV)\n",
    "\n",
    "# Loading weights for new model\n",
    "weights[0] = weights_t\n",
    "lenet_model.layers[fc_id].set_weights(weights)\n",
    "\n",
    "# Write the testing input and output variables\n",
    "score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "slr_accuracy = score[1]\n",
    "print('Params:',tU.size+tS.size+tV.size,', Acc:', slr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_10 (Averag (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1210      \n",
      "=================================================================\n",
      "Total params: 51,902\n",
      "Trainable params: 51,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "fc1 shape (400, 120)\n",
      "(400, 120)\n",
      "(120,)\n",
      "(120, 120)\n",
      "(400, 8)\n",
      "(8,)\n",
      "(8, 120)\n",
      "(400, 120)\n",
      "48000\n",
      "4168   0.9492999911308289\n"
     ]
    }
   ],
   "source": [
    "#LENET_SVD\n",
    "from keras import backend\n",
    "from keras import datasets\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.models import Model,Sequential, model_from_json\n",
    "from keras.layers import Dense, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "fc_id = 5 # FC Layer Number\n",
    "rank = 8\n",
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1:]\n",
    "\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 32,32,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32,32,1) \n",
    "\n",
    "json_file = open('Lenet.json', 'r')\n",
    "lenet_model_json = json_file.read()\n",
    "json_file.close()\n",
    "lenet_model = model_from_json(lenet_model_json)\n",
    "lenet_model.load_weights(\"Lenet.h5\")\n",
    "lenet_model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "\n",
    "lenet_model.summary()\n",
    "\n",
    "# Loading weights of the model\n",
    "keep = rank\n",
    "fc1 = lenet_model.layers[fc_id].get_weights()\n",
    "\n",
    "print('fc1 shape',fc1[0].shape)\n",
    "# Decomposition and Reconstruction\n",
    "U, S, V = np.linalg.svd(fc1[0], full_matrices=False)\n",
    "print(U.shape)\n",
    "print(S.shape)\n",
    "print(V.shape)\n",
    "tU, tS, tV = U[:, 0:keep], S[0:keep], V[0:keep, :]\n",
    "print(tU.shape)\n",
    "print(tS.shape)\n",
    "print(tV.shape)\n",
    "\n",
    "\n",
    "fc1_t = np.matmul(np.matmul(tU, np.diag(tS)), tV)\n",
    "print(fc1_t.shape)\n",
    "\n",
    "print(np.count_nonzero(fc1_t))\n",
    "\n",
    "# Loading weights for new model\n",
    "fc1[0] = fc1_t\n",
    "lenet_model.layers[fc_id].set_weights(fc1)\n",
    "\n",
    "\n",
    "# Write the testing input and output variables\n",
    "score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "truncsvd_accuracy = score[1]\n",
    "print(tU.size+tS.size+tV.size,' ', truncsvd_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LENET_TRAIN\n",
    "from keras import backend\n",
    "from keras import datasets\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1:]\n",
    "\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 32,32,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32,32,1) \n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential()\n",
    "\n",
    "# C1 Convolutional Layer\n",
    "model.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(32,32,1)))\n",
    "\n",
    "# S2 Pooling Layer\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "# C3 Convolutional Layer\n",
    "model.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "# S4 Pooling Layer\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "#Flatten the CNN output so that we can connect it with fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# C5 Fully Connected Convolutional Layer\n",
    "model.add(layers.Dense(120, activation='tanh'))\n",
    "\n",
    "\n",
    "#Output Layer with softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='Adam', metrics=[\"accuracy\"]) \n",
    "model.summary()\n",
    "\n",
    "\n",
    "hist = model.fit(x=x_train,y=y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=1) \n",
    "\n",
    "# Write the testing input and output variables\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "\n",
    "# Write the file name of the model\n",
    "\n",
    "with open(\"Lenet.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "# Write the file name of the weights\n",
    "\n",
    "model.save_weights(\"Lenet.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
